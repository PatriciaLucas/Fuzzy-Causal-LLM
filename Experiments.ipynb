{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": ""
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 11654772,
          "sourceType": "datasetVersion",
          "datasetId": 7314122
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table style=\"margin: auto; background-color: white;\">\n",
        "    <tr>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1lgflViz1uefcvVW1iI57haB4M1bKsZtp' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "      <td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1S3xpVbkzpBAG51PyuRyIioecvZiHGSap' alt=\"drawing\" width=\"200\" />\n",
        "      </td>\n",
        "    </tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "Z-B119x0wOMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Causal Graph Fuzzy LLM**\n",
        "\n",
        "\n",
        "<td style=\"background-color: white;\">\n",
        "        <img src='https://drive.google.com/uc?export=view&id=1igs_5aIACphsNn2oZiE10-nRFvnSXv-K' alt=\"drawing\" width=\"2000\" />\n",
        "</td>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oumWebICweUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet git+https://github.com/PatriciaLucas/AutoML.git\n",
        "\n",
        "!pip install --quiet git+https://PatriciaLucas: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@github.com/petroniocandido/clshq_tk.git\n",
        "\n",
        "!git clone https://github.com/PatriciaLucas/Fuzzy-Causal-LLM.git"
      ],
      "metadata": {
        "trusted": true,
        "id": "LD2wUQCunCM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803a8afa-7454-4b47-b253-c348478c8289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.6/309.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.4/259.4 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoCDE-TS (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import clshq_tk\n",
        "from AUTODCETS import util, feature_selection, datasets, save_database as sd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from clshq_tk.modules.fuzzy import GridPartitioner, trimf, gaussmf, training_loop\n",
        "from clshq_tk.data.regression import RegressionTS\n",
        "from clshq_tk.common import DEVICE, DEFAULT_PATH, resume, checkpoint, order_window\n",
        "\n",
        "import sys\n",
        "sys.path.append('Fuzzy-Causal-LLM')\n",
        "import fuzzy_causal_text as fcllm"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T20:30:28.095125Z",
          "iopub.execute_input": "2025-05-04T20:30:28.095708Z",
          "iopub.status.idle": "2025-05-04T20:30:55.634862Z",
          "shell.execute_reply.started": "2025-05-04T20:30:28.095675Z",
          "shell.execute_reply": "2025-05-04T20:30:55.634078Z"
        },
        "id": "_Gf0bm1bnCM3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Datasets**:\n",
        "\n",
        "**1.   CLIMATIC_1:** SONDA --- TARGET: glo_avg --- 35.000 SAMPLES --- 12 VARIABLES\n",
        "\n",
        "**2.   ENERGY_1**: WIND ENERGY PRODUCTION --- TARGET: Power --- 43.800 SAMPLES --- 9 VARIABLES\n",
        "\n",
        "**3.   IOT_1:** HOUSEHOLD ELECTRICITY CONSUMPTION IN MEXICO --- TARGET: active_power --- 100.000 SAMPLES --- 14 VARIABLES\n",
        "\n",
        "**4.   ECONOMICS_1:** BITCOIN --- TARGET: AVG --- 2.970 SAMPLES --- 06 VARIABLES\n",
        "\n"
      ],
      "metadata": {
        "id": "RvVR3cAJvOsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name_dataset = 'IOT_1'\n",
        "target = 'active_power'\n",
        "df = fcllm.upload_dataset(name_dataset)\n",
        "\n",
        "if name_dataset == 'CLIMATIC_1':\n",
        "  df = df.head(35000)\n",
        "elif name_dataset == 'ENERGY_1':\n",
        "  df = df.head(43800)\n",
        "elif name_dataset == 'IOT_1':\n",
        "  df = df.head(100000)\n",
        "else:\n",
        "  df = df.head(2970)\n",
        "\n",
        "max_lags = 20\n",
        "partitions = 30\n",
        "path_model = 'model'\n",
        "database_path = 'database_text.db'\n",
        "epochs = 20\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "name_model = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(name_model)\n",
        "\n",
        "windows = fcllm.rolling_window(df, 10)\n",
        "\n",
        "sd.execute(\"CREATE TABLE IF NOT EXISTS results(name_dataset TEXT, window INT, predict FLOAT, \\\n",
        "               real FLOAT)\", database_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T23:37:11.594286Z",
          "iopub.execute_input": "2025-05-04T23:37:11.594648Z",
          "iopub.status.idle": "2025-05-04T23:37:13.663002Z",
          "shell.execute_reply.started": "2025-05-04T23:37:11.594624Z",
          "shell.execute_reply": "2025-05-04T23:37:13.662333Z"
        },
        "id": "s-3bxJRvnCM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72c22ea-d3eb-488a-f5d1-0be2c3162484"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The experiment was conducted to evaluate the methods across $10$ windows of size $0.3 * |D|$ with a $30\\%$ overlap throughout the multivariate time series of sample size $|D|$. The windows were divided into training and test sets."
      ],
      "metadata": {
        "id": "UdsUCROq1ceA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the windows\n",
        "for i, window in enumerate(windows):\n",
        "      if i < 0:  # Enter the number of the last window saved in the database here\n",
        "        print(\"window already executed\")\n",
        "\n",
        "      else:\n",
        "\n",
        "        # Exclude constant series.\n",
        "        for variable in window.columns:\n",
        "            if window[variable].max() == window[variable].min():\n",
        "                window = window.drop(variable, axis=1)\n",
        "                print(f\"Variables {variable} were deleted because they are constant.\")\n",
        "\n",
        "        # Fuzzy-Causal\n",
        "        ds, scaler = fcllm.fuzzy_causal(window, name_dataset, target, max_lags, tokenizer, partitions)\n",
        "\n",
        "        # Text\n",
        "        ds, scaler = fcllm.text(window, name_dataset, target, max_lags, tokenizer)\n",
        "\n",
        "        # Text-Causal\n",
        "        ds, scaler = fcllm.causal_text(window, name_dataset, target, max_lags, tokenizer)\n",
        "\n",
        "        train_dataset, test_dataset = torch.utils.data.random_split(ds, [0.80, 0.20])\n",
        "\n",
        "        freeze = True # Freeze GPT-2 weights\n",
        "        model = fcllm.train_model(train_dataset, name_model, epochs, scaler, freeze, path_model = None)\n",
        "\n",
        "        forecasts, real = fcllm.predict(test_dataset, model, tokenizer, target, scaler)\n",
        "\n",
        "        for j in range(len(forecasts)):\n",
        "            sd.execute_insert(\"INSERT INTO results VALUES(?, ?, ?, ?)\", (name_dataset, i, forecasts[j], real[j]), database_path)\n",
        "\n",
        "        print(f'Save window: {i}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-04T23:37:20.531157Z",
          "iopub.execute_input": "2025-05-04T23:37:20.531458Z",
          "iopub.status.idle": "2025-05-05T00:40:52.065306Z",
          "shell.execute_reply.started": "2025-05-04T23:37:20.531438Z",
          "shell.execute_reply": "2025-05-05T00:40:52.064369Z"
        },
        "id": "WbZBWFCbnCM5"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}